{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##THEORETICAL"
      ],
      "metadata": {
        "id": "Pr15qaoUHtBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Can we use Bagging for regression problems.\n"
      ],
      "metadata": {
        "id": "wSbcLVv677Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Yes, Bagging can be used for regression problems. In Bagging, multiple models are trained independently on random subsets of the data, and for regression, the predictions of all models are averaged to produce a final prediction. This helps in reducing variance and improving model performance.'''"
      ],
      "metadata": {
        "id": "kK6dOmNqCIRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between multiple model training and single model training.\n"
      ],
      "metadata": {
        "id": "qMJPQDSV77A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Multiple Model Training: Involves training several models on the same dataset or different subsets of the data. The predictions from these models are often combined using techniques like Bagging or Boosting.\n",
        "Single Model Training: Involves training only one model on the entire dataset. It is a more straightforward approach but may not capture as much variation or complexity as ensemble methods.\n",
        "'''"
      ],
      "metadata": {
        "id": "K7PI-WtwCO0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Explain the concept of feature randomness in Random Forest.\n"
      ],
      "metadata": {
        "id": "hXJFppvk8qPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Feature randomness in Random Forest refers to the practice of selecting a random subset of features (attributes) for each decision tree during the training process. This introduces diversity among the trees, ensuring that they are less correlated, and helps in reducing overfitting.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "TlXG6GsmCSqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is OOB (Out-of-Bag) Score.\n"
      ],
      "metadata": {
        "id": "4fLKZJSz76_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The OOB score is a method of evaluating the performance of a Random Forest model. It uses data points that were not included in the bootstrap sample (the out-of-bag samples) to test the model. This provides an internal validation set, offering a way to estimate model accuracy without needing a separate test set.'''"
      ],
      "metadata": {
        "id": "93a5eEjKCV8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How can you measure the importance of features in a Random Forest model.\n"
      ],
      "metadata": {
        "id": "7BA8EHTu76-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Feature importance in Random Forest can be measured using metrics like:\n",
        "\n",
        "Gini Importance: Based on how well a feature reduces the impurity in the trees.\n",
        "Permutation Importance: By measuring the decrease in model performance when the values of a feature are randomly permuted.\n",
        "'''"
      ],
      "metadata": {
        "id": "9O4_T2OJCb9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Explain the working principle of a Bagging Classifier.\n"
      ],
      "metadata": {
        "id": "-SGlEsBj769P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''A Bagging Classifier works by training multiple classifiers (usually decision trees) on different random subsets of the training data (with replacement). After the models are trained, their predictions are combined through majority voting (for classification) to make the final prediction.'''"
      ],
      "metadata": {
        "id": "XtGsQ7hxCiDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.How do you evaluate a Bagging Classifierâ€™s performance.\n"
      ],
      "metadata": {
        "id": "_v1RJmMU765g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The performance of a Bagging Classifier is typically evaluated using metrics such as:\n",
        "\n",
        "Accuracy\n",
        "Precision, recall, and F1-score (for imbalanced data)\n",
        "Confusion matrix\n",
        "Cross-validation'''"
      ],
      "metadata": {
        "id": "8xhj6HO0DBj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.How does a Bagging Regressor work.\n"
      ],
      "metadata": {
        "id": "S-fGB7lJ762M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''A Bagging Regressor works similarly to a Bagging Classifier but is designed for regression tasks. It trains multiple regression models on random subsets of the data and averages their predictions to produce the final output.'''"
      ],
      "metadata": {
        "id": "HKRtGjznDELN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is the main advantage of ensemble techniques.\n"
      ],
      "metadata": {
        "id": "GsnWNCGJ76zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The main advantage of ensemble techniques is that they improve the overall performance of the model by combining the strengths of multiple base models, thereby reducing variance, bias, or both. This leads to better generalization and robustness.'''"
      ],
      "metadata": {
        "id": "M16e7roxDG9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What is the main challenge of ensemble methods.\n"
      ],
      "metadata": {
        "id": "LX_7g1-P76uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The main challenge of ensemble methods is their complexity. They require more computational resources and are harder to interpret than individual models. Additionally, overfitting can occur if not properly tuned or if too many weak models are combined.'''"
      ],
      "metadata": {
        "id": "CaJjBfXP9VKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Explain the key idea behind ensemble techniques.\n"
      ],
      "metadata": {
        "id": "yVAopaOy76sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The key idea behind ensemble techniques is to combine multiple models to make a final prediction. The goal is to leverage the diversity of the models to improve predictive accuracy, robustness, and generalization by reducing errors such as variance and bias.'''"
      ],
      "metadata": {
        "id": "X-71wCH2DMNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is a Random Forest Classifier.\n"
      ],
      "metadata": {
        "id": "dk6ynOsN76jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''A Random Forest Classifier is an ensemble learning method based on decision trees. It constructs a multitude of decision trees during training and outputs the class that is the mode of the classes (for classification tasks) of the individual trees.'''"
      ],
      "metadata": {
        "id": "8z-ahytrDO7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What are the main types of ensemble techniques.\n"
      ],
      "metadata": {
        "id": "uD0Dxbf_9WBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The main types of ensemble techniques are:\n",
        "\n",
        "Bagging: Builds multiple models using different subsets of data and combines their outputs.\n",
        "Boosting: Sequentially builds models, where each new model corrects the errors of the previous ones.\n",
        "Stacking: Combines multiple different models (often heterogeneous) to make predictions.'''"
      ],
      "metadata": {
        "id": "yEHGK0BDDR08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What is ensemble learning in machine learning.\n"
      ],
      "metadata": {
        "id": "y_ggvWzE9V99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Ensemble learning is a technique in machine learning where multiple models (often called base learners) are combined to solve a problem. The purpose is to improve the overall performance by reducing the risk of overfitting and increasing accuracy.'''"
      ],
      "metadata": {
        "id": "6RHZ45ndDUX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.When should we avoid using ensemble methods.\n"
      ],
      "metadata": {
        "id": "inSdh_IZ9V7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Ensemble methods should be avoided when:\n",
        "\n",
        "The model needs to be interpretable (e.g., decision trees are more interpretable than ensemble models).\n",
        "Computational resources are limited, as ensemble models can be expensive to train and deploy.\n",
        "The dataset is very small, where ensemble models may not offer significant improvement.'''"
      ],
      "metadata": {
        "id": "_MRtDGKFDYek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.How does Bagging help in reducing overfitting.\n"
      ],
      "metadata": {
        "id": "BkLPXcwM9V4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Bagging helps in reducing overfitting by training multiple models on different subsets of the data. By averaging the predictions, Bagging reduces the variance and prevents the model from learning overly complex patterns specific to the training data.'''"
      ],
      "metadata": {
        "id": "P4rM3u7IDbTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.Why is Random Forest better than a single Decision Tree.\n"
      ],
      "metadata": {
        "id": "XqstUw2p9V2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Random Forest is better than a single Decision Tree because it combines multiple trees to make predictions. This reduces the risk of overfitting that occurs in a single Decision Tree, where the model may fit too closely to the training data. Random Forest introduces randomness in both data and features, making it more robust.'''"
      ],
      "metadata": {
        "id": "WZG-KNnvDd3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What is the role of bootstrap sampling in Bagging.\n"
      ],
      "metadata": {
        "id": "3vxxJn9U9VzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Bootstrap sampling involves randomly selecting subsets of data (with replacement) for training each model in the ensemble. This technique ensures that each model sees a different view of the data, introducing variability and reducing overfitting.'''"
      ],
      "metadata": {
        "id": "FSwd7txvDgu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What are some real-world applications of ensemble techniques.\n"
      ],
      "metadata": {
        "id": "aN6w61659Vwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Real-world applications of ensemble techniques include:\n",
        "\n",
        "Healthcare: Disease prediction and diagnosis.\n",
        "Finance: Fraud detection and risk assessment.\n",
        "Marketing: Customer segmentation and churn prediction.\n",
        "Retail: Sales forecasting and inventory management.'''"
      ],
      "metadata": {
        "id": "LDLBiCCTDk5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "icRzivZW9pZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGhiRDSg6iAS"
      },
      "outputs": [],
      "source": [
        "'''Bagging: Builds multiple models independently on random subsets of data and combines their outputs. It reduces variance and works well with high-variance models like decision trees.\n",
        "Boosting: Builds models sequentially, where each model corrects the errors of the previous ones. It reduces both bias and variance and tends to work well for improving weak models.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PRACTICAL"
      ],
      "metadata": {
        "id": "h8EhH-jXD9s0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n"
      ],
      "metadata": {
        "id": "YbYZUsXu90Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOR7Pe_jECNW",
        "outputId": "7f38a300-f263-4c4c-f381-bad55a0ed4a5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n"
      ],
      "metadata": {
        "id": "56JzCNVn90U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate MSE\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mxEkDgaEE-A",
        "outputId": "e17218b0-a78e-470e-e9c9-a920e29654fd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 3181.6743862298945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n"
      ],
      "metadata": {
        "id": "zR5wKFyn90Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X, y)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = pd.Series(rf_clf.feature_importances_, index=data.feature_names)\n",
        "print(feature_importances.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsYfp-ceEHVb",
        "outputId": "4602c5e8-cbfb-475b-ea75-6459a93d7955"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst area                 0.139357\n",
            "worst concave points       0.132225\n",
            "mean concave points        0.107046\n",
            "worst radius               0.082848\n",
            "worst perimeter            0.080850\n",
            "mean perimeter             0.067990\n",
            "mean concavity             0.066917\n",
            "mean area                  0.060462\n",
            "worst concavity            0.037339\n",
            "mean radius                0.034843\n",
            "area error                 0.029553\n",
            "worst compactness          0.019864\n",
            "worst texture              0.017485\n",
            "mean texture               0.015225\n",
            "radius error               0.014264\n",
            "worst smoothness           0.012232\n",
            "mean compactness           0.011597\n",
            "perimeter error            0.010085\n",
            "worst symmetry             0.008179\n",
            "mean smoothness            0.007958\n",
            "fractal dimension error    0.005942\n",
            "concavity error            0.005820\n",
            "compactness error          0.005612\n",
            "smoothness error           0.004722\n",
            "worst fractal dimension    0.004497\n",
            "concave points error       0.003760\n",
            "texture error              0.003744\n",
            "symmetry error             0.003546\n",
            "mean symmetry              0.003423\n",
            "mean fractal dimension     0.002615\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n"
      ],
      "metadata": {
        "id": "nZTb63PP90QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "dt_pred = dt_reg.predict(X_test)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "# Compare Mean Squared Error\n",
        "print(\"Decision Tree MSE:\", mean_squared_error(y_test, dt_pred))\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rJsHnT9EOPl",
        "outputId": "2354ce73-1e8d-4782-bb88-f655ce3e473d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 6413.149622812587\n",
            "Random Forest MSE: 2609.093889983524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n"
      ],
      "metadata": {
        "id": "6WojE7gO90NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier with OOB Score\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_clf.fit(X, y)\n",
        "\n",
        "# Print OOB Score\n",
        "print(\"OOB Score:\", rf_clf.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1pSvvctET7s",
        "outputId": "f13fadc8-3d84-410e-f700-2f635290a241"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n"
      ],
      "metadata": {
        "id": "mdcT71dV90Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier with SVM\n",
        "# The 'base_estimator' argument has been replaced with 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZTUGWgGEYJp",
        "outputId": "e2d28243-7872-45f9-a6c2-6651e7f91cc6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n"
      ],
      "metadata": {
        "id": "YIl0rpzG90H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate Random Forest with different numbers of trees\n",
        "for n_trees in [10, 50, 100, 200]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    print(f\"Accuracy with {n_trees} trees:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHCos1zOEagx",
        "outputId": "37a36705-7b0b-48cf-9ead-bb38983bc78e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 10 trees: 0.855\n",
            "Accuracy with 50 trees: 0.88\n",
            "Accuracy with 100 trees: 0.9\n",
            "Accuracy with 200 trees: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n"
      ],
      "metadata": {
        "id": "xJFVfNTP90FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier with Logistic Regression\n",
        "# The 'base_estimator' argument has been replaced with 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=LogisticRegression(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and compute AUC score\n",
        "y_prob = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "print(\"AUC Score:\", roc_auc_score(y_test, y_prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KfwsekHEdAP",
        "outputId": "fc100b71-c384-4ec5-febf-0b62a6dd1a82"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9225203497135966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.Train a Random Forest Regressor and analyze feature importance scores.\n"
      ],
      "metadata": {
        "id": "PJFiIcqd90Cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "\n",
        "# Create synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X, y)\n",
        "\n",
        "# Print feature importance scores\n",
        "feature_importances = pd.Series(rf_reg.feature_importances_)\n",
        "print(feature_importances.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TYgWP58EfID",
        "outputId": "74c1cdb2-7e7a-4626-e153-27d9bb1760a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3    0.337452\n",
            "6    0.290048\n",
            "9    0.203566\n",
            "0    0.041871\n",
            "2    0.038493\n",
            "1    0.035598\n",
            "5    0.015657\n",
            "7    0.015004\n",
            "4    0.011310\n",
            "8    0.011000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n"
      ],
      "metadata": {
        "id": "tyHnY88m9z_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "# The 'base_estimator' argument has been replaced with 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "# Compare accuracies\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred_bagging))\n",
        "print(\"Random Forest Classifier Accuracy:\", accuracy_score(y_test, y_pred_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YtxTiO_Et4w",
        "outputId": "9b74b4e1-0874-427a-98a1-d9df37e6d312"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.885\n",
            "Random Forest Classifier Accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n"
      ],
      "metadata": {
        "id": "CUEhZ5ff9z9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [None, 10, 20]}\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7fUyYI-EwUO",
        "outputId": "dec51ee8-6d0e-4787-e46f-bf442debe486"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 10, 'n_estimators': 150}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32.Train a Bagging Regressor with different numbers of base estimators and compare performance.\n"
      ],
      "metadata": {
        "id": "0WviszUX9z6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate Bagging Regressor with different numbers of base estimators\n",
        "for n_estimators in [5, 10, 20, 50]:\n",
        "    # The 'base_estimator' argument has been replaced with 'estimator'\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    print(f\"MSE with {n_estimators} estimators:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlTWBddGEzMs",
        "outputId": "3edbf21c-f6dd-4798-f7ad-70c1c05e36d0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE with 5 estimators: 4023.5638830660623\n",
            "MSE with 10 estimators: 3181.6743862298945\n",
            "MSE with 20 estimators: 2937.774090111561\n",
            "MSE with 50 estimators: 2594.42700538487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33.Train a Random Forest Classifier and analyze misclassified samples.\n"
      ],
      "metadata": {
        "id": "hwTa8azO9z3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Identify misclassified samples\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "misclassified_samples = np.where(y_pred != y_test)[0]\n",
        "print(\"Number of misclassified samples:\", len(misclassified_samples))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuN6POGnE3TD",
        "outputId": "d57c7390-394a-4c8a-835f-be6ed9de5a57"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of misclassified samples: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n"
      ],
      "metadata": {
        "id": "RkDlmec19z05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "# The 'base_estimator' argument has been replaced with 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "\n",
        "# Compare accuracies\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred_bagging))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qztOeiUE52C",
        "outputId": "4522cdc8-f28c-45cf-c4fe-24aacfc89c0c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.875\n",
            "Bagging Classifier Accuracy: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35.Train a Random Forest Classifier and visualize the confusion matrix.\n"
      ],
      "metadata": {
        "id": "pXwtzvAI9zyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and plot confusion matrix\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "kr2SmItrFAuZ",
        "outputId": "35aa6f0c-dd92-4d71-f82b-e20f50d87b34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36.Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n"
      ],
      "metadata": {
        "id": "PthqAqW99zvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True))\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stack_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
        "stack_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute accuracy\n",
        "y_pred = stack_clf.predict(X_test)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn1VTzNpFEQ4",
        "outputId": "57eca30e-eb23-49f6-c8bc-db543590932e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37.Train a Random Forest Classifier and print the top 5 most important features.\n"
      ],
      "metadata": {
        "id": "0dDC6ifz9zsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X, y)\n",
        "\n",
        "# Print top 5 most important features\n",
        "feature_importances = pd.Series(rf_clf.feature_importances_).sort_values(ascending=False)\n",
        "print(\"Top 5 most important features:\\n\", feature_importances.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppJccXQGFGks",
        "outputId": "dae1a70d-0a9e-4244-eada-9c428faa895b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            " 5     0.322934\n",
            "18    0.207074\n",
            "1     0.095728\n",
            "14    0.082296\n",
            "11    0.029221\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "4yW7jwR79zpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "# The 'base_estimator' argument has been replaced with 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute metrics\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1-score:\", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_ByaLDgFI1x",
        "outputId": "4f56415d-41e5-4960-e989-5fd08ed43fe0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9565217391304348\n",
            "Recall: 0.822429906542056\n",
            "F1-score: 0.8844221105527639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "39.Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n"
      ],
      "metadata": {
        "id": "bCenDULd9znS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate Random Forest with different max_depth values\n",
        "for depth in [5, 10, 20, None]:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    print(f\"Accuracy with max_depth={depth}:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8MKaHtzFN-D",
        "outputId": "532bfb30-5d0f-4412-bdbc-56fb9273ea56"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=5: 0.88\n",
            "Accuracy with max_depth=10: 0.885\n",
            "Accuracy with max_depth=20: 0.9\n",
            "Accuracy with max_depth=None: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40.Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n"
      ],
      "metadata": {
        "id": "a6zq41Xb9zkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate Bagging Regressors with different base estimators\n",
        "for base_estimator in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
        "    # The 'base_estimator' argument has been replaced with 'estimator'\n",
        "    bagging_reg = BaggingRegressor(estimator=base_estimator, n_estimators=10, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    print(f\"MSE with {base_estimator.__class__.__name__}:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zkOQkp6FQxV",
        "outputId": "82e7a5ce-901d-4011-cb45-a0bf1a8d83c7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE with DecisionTreeRegressor: 3181.6743862298945\n",
            "MSE with KNeighborsRegressor: 3581.2345261433843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n"
      ],
      "metadata": {
        "id": "JX6MPa749zhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and compute ROC-AUC score\n",
        "y_prob = rf_clf.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOcnGL2EFSzL",
        "outputId": "b643b034-af23-4f79-f4aa-a6333955a082"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9379459350819013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42.Train a Bagging Classifier and evaluate its performance using cross-validatio.\n"
      ],
      "metadata": {
        "id": "IaKgv8Co9zeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier with cross-validation\n",
        "# The 'base_estimator' argument has been replaced with 'estimator'\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "scores = cross_val_score(bagging_clf, X, y, cv=5)\n",
        "\n",
        "# Print average accuracy\n",
        "print(\"Cross-validation Accuracy:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M5wLgAoFVrm",
        "outputId": "78dde224-e024-41ce-c84b-417179bef251"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation Accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.Train a Random Forest Classifier and plot the Precision-Recall curv.\n",
        "44.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
        "45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "81NDzxQXBJ88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and plot Precision-Recall curve\n",
        "y_prob = rf_clf.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "llcvjkVGFXzS",
        "outputId": "64429efb-94f5-4691-9ec0-f197e211acb6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "44.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n"
      ],
      "metadata": {
        "id": "BKLvBDfzBNLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=50, random_state=42))\n",
        "]\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stack_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
        "stack_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute accuracy\n",
        "y_pred = stack_clf.predict(X_test)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJw3LTLMFaA4",
        "outputId": "5d58e8f0-397b-495c-f9b4-e50c943a9298"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Classifier Accuracy: 0.895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "gpdDHtyvBNrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate Bagging Regressor with different bootstrap sample sizes\n",
        "for bootstrap_samples in [0.5, 0.7, 1.0]:  # Proportion of training set used in each bootstrap sample\n",
        "    # The 'base_estimator' argument has been replaced with 'estimator'\n",
        "    bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10,\n",
        "                                   max_samples=bootstrap_samples, random_state=42)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    print(f\"MSE with {bootstrap_samples*100}% bootstrap samples:\", mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSPsiFvQFczP",
        "outputId": "e2f77f56-a0e9-4089-cf51-5131f8fd8608"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE with 50.0% bootstrap samples: 3196.837141219787\n",
            "MSE with 70.0% bootstrap samples: 3323.423822544224\n",
            "MSE with 100.0% bootstrap samples: 3181.6743862298945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O88VHpxOFdSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}